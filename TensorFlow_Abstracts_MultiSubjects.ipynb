{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow on Abstracts of Multiple Subjects from August 2017 Training Data \n",
    "In this notebook, a recurrent neural network is used to identify disciplines from journal abstracts. Using a RNN is useful and more accurate since information about the *sequence* of the words is used. For this project, a dataset of titles (features) and subjects (labels) from scientific papers is used for the RNN training. This RNN uses an *embedding layer* for a more efficient representation of the vocabulary. This new representation of words from the embedding layer is then passed to Long-Short Term Memory (LSTM) cells, which adds recurrent connections to the newtork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Main Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Preprocess\n",
    "The original data file from the Google shared drive 'arxiv_train_set' was modified to include column labels ('Titles' and 'Labels') and saved as a csv file using Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstracts</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We present a semi-analytical model of star for...</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the evolution in the halo mass function along ...</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We consider a general class of vector-tensor t...</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-modes in CMB polarization from patchy reioni...</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We study the effect of noise on the evolution ...</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Abstracts       Labels\n",
       "0  We present a semi-analytical model of star for...  astro-ph.CO\n",
       "1  the evolution in the halo mass function along ...  astro-ph.CO\n",
       "2  We consider a general class of vector-tensor t...  astro-ph.CO\n",
       "3  B-modes in CMB polarization from patchy reioni...  astro-ph.CO\n",
       "4  We study the effect of noise on the evolution ...  astro-ph.CO"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('arxiv_train_set_abstracts.csv', encoding='latin1') ## Encoding of latin1 helps eliminate reading errors\n",
    "data.head() ## Inspect data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We present a semi-analytical model of star formation which explains simultaneously the observed UV luminosity function of high redshift Lyman break galaxies (LBGs) and luminosity functions of Lyman-alpha emitters. We consider both models that use the Press-Schechter (PS) and Sheth-Tormen (ST) halo mass functions to calculate the abundances of dark matter halos. The Lyman-alpha luminosity functions at z < 4 are well reproduced with only <10% of the LBGs emitting Lyman-alpha lines with rest equivalent width greater than the limiting equivalent width of the narrow band surveys. However, the observed luminosity function at z > 5 can be reproduced only when we assume that nearly all LBGs are Lyman-alpha emitters. Thus it appears that 4 < z < 5 marks the epoch when a clear change occurs in the physical properties of the high redshift galaxies. As Lyman-alpha escape depends on dust and gas kinematics of the inter stellar medium (ISM), this could mean that on an average the ISM at z > 5 could be less dusty, more clumpy and having more complex velocity field. All of these will enable easier escape of the Lyman-alpha photons. At z > 5 the observed Lyman-alpha luminosity function are well reproduced with'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define titles and labels \n",
    "abstracts = data.Abstracts.astype(str)\n",
    "labels = data.Labels.astype(str)\n",
    "\n",
    "# Inspect titles\n",
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Abstracts\n",
    "As shown in the abstract example, there are capitilazations, punctuations, and formatting issues that need to be cleaned up before creating an embedding layer. Hyphenated words, for now, will be made into one word. It may be useful to reconsider this approach on hyphenated words if it is necessary to improve accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we present a semianalytical model of star formation which explains simultaneously the observed uv luminosity function of high redshift lyman break galaxies lbgs and luminosity functions of lymanalpha emitters we consider both models that use the pressschechter ps and shethtormen st halo mass functions to calculate the abundances of dark matter halos the lymanalpha luminosity functions at z  4 are well reproduced with only 10 of the lbgs emitting lymanalpha lines with rest equivalent width greater than the limiting equivalent width of the narrow band surveys however the observed luminosity function at z  5 can be reproduced only when we assume that nearly all lbgs are lymanalpha emitters thus it appears that 4  z  5 marks the epoch when a clear change occurs in the physical properties of the high redshift galaxies as lymanalpha escape depends on dust and gas kinematics of the inter stellar medium ism this could mean that on an average the ism at z  5 could be less dusty more clumpy and having more complex velocity field all of these will enable easier escape of the lymanalpha photons at z  5 the observed lymanalpha luminosity function are well reproduced with'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clean up abstracts\n",
    "import re\n",
    "from string import punctuation \n",
    "\n",
    "abstracts = abstracts.replace('\\r', '', regex=True) ## Remove '\\r'\n",
    "\n",
    "abstracts_clean = []\n",
    "for row in abstracts:\n",
    "    row = row.lower() ## Put all text into lowercase\n",
    "    row = ''.join([c for c in row if c not in punctuation]) ## Remove punctuation\n",
    "    abstracts_clean.append(row)\n",
    "    \n",
    "## Inspect clean titles\n",
    "abstracts_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocaublary and transforming words into integers\n",
    "This step is necessary for TensorFlow to work with words of the abstracts in a numerical manner. All words are given a specific *token* that is used to give them a numeric presence. The value of the token does not necesarilly correlate with the length, frequency, or position of the respective word. However, it is common to sort the words by frequency to have a way to apply cutoffs in the vocabulary if needed to imporve training accuracies. An inversion dictionary was added to convert integers back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 56,\n",
       " 3,\n",
       " 7555,\n",
       " 22,\n",
       " 2,\n",
       " 667,\n",
       " 294,\n",
       " 19,\n",
       " 2930,\n",
       " 1213,\n",
       " 1,\n",
       " 157,\n",
       " 2862,\n",
       " 1622,\n",
       " 73,\n",
       " 2,\n",
       " 106,\n",
       " 2265,\n",
       " 9746,\n",
       " 3590,\n",
       " 852,\n",
       " 20853,\n",
       " 4,\n",
       " 1622,\n",
       " 113,\n",
       " 2,\n",
       " 10130,\n",
       " 8079,\n",
       " 8,\n",
       " 140,\n",
       " 59,\n",
       " 51,\n",
       " 10,\n",
       " 71,\n",
       " 1,\n",
       " 39649,\n",
       " 4483,\n",
       " 4,\n",
       " 58985,\n",
       " 7412,\n",
       " 2141,\n",
       " 199,\n",
       " 113,\n",
       " 6,\n",
       " 902,\n",
       " 1,\n",
       " 4476,\n",
       " 2,\n",
       " 1109,\n",
       " 557,\n",
       " 5191,\n",
       " 1,\n",
       " 10130,\n",
       " 1622,\n",
       " 113,\n",
       " 24,\n",
       " 1220,\n",
       " 920,\n",
       " 14,\n",
       " 88,\n",
       " 3019,\n",
       " 11,\n",
       " 79,\n",
       " 961,\n",
       " 2,\n",
       " 1,\n",
       " 20853,\n",
       " 7206,\n",
       " 10130,\n",
       " 908,\n",
       " 11,\n",
       " 2817,\n",
       " 630,\n",
       " 1640,\n",
       " 1551,\n",
       " 67,\n",
       " 1,\n",
       " 2055,\n",
       " 630,\n",
       " 1640,\n",
       " 2,\n",
       " 1,\n",
       " 2344,\n",
       " 1396,\n",
       " 2221,\n",
       " 134,\n",
       " 1,\n",
       " 157,\n",
       " 1622,\n",
       " 73,\n",
       " 24,\n",
       " 1220,\n",
       " 1057,\n",
       " 21,\n",
       " 18,\n",
       " 3019,\n",
       " 79,\n",
       " 64,\n",
       " 8,\n",
       " 1615,\n",
       " 10,\n",
       " 1517,\n",
       " 55,\n",
       " 20853,\n",
       " 14,\n",
       " 10130,\n",
       " 8079,\n",
       " 240,\n",
       " 23,\n",
       " 1227,\n",
       " 10,\n",
       " 920,\n",
       " 1220,\n",
       " 1057,\n",
       " 5421,\n",
       " 1,\n",
       " 4526,\n",
       " 64,\n",
       " 3,\n",
       " 1535,\n",
       " 522,\n",
       " 1273,\n",
       " 5,\n",
       " 1,\n",
       " 332,\n",
       " 85,\n",
       " 2,\n",
       " 1,\n",
       " 106,\n",
       " 2265,\n",
       " 852,\n",
       " 16,\n",
       " 10130,\n",
       " 5004,\n",
       " 692,\n",
       " 13,\n",
       " 1393,\n",
       " 4,\n",
       " 492,\n",
       " 5591,\n",
       " 2,\n",
       " 1,\n",
       " 6492,\n",
       " 934,\n",
       " 973,\n",
       " 5569,\n",
       " 12,\n",
       " 334,\n",
       " 339,\n",
       " 10,\n",
       " 13,\n",
       " 17,\n",
       " 397,\n",
       " 1,\n",
       " 5569,\n",
       " 24,\n",
       " 1220,\n",
       " 1057,\n",
       " 334,\n",
       " 18,\n",
       " 544,\n",
       " 5226,\n",
       " 65,\n",
       " 9303,\n",
       " 4,\n",
       " 871,\n",
       " 65,\n",
       " 141,\n",
       " 568,\n",
       " 69,\n",
       " 55,\n",
       " 2,\n",
       " 27,\n",
       " 148,\n",
       " 1433,\n",
       " 3126,\n",
       " 5004,\n",
       " 2,\n",
       " 1,\n",
       " 10130,\n",
       " 1891,\n",
       " 24,\n",
       " 1220,\n",
       " 1057,\n",
       " 1,\n",
       " 157,\n",
       " 10130,\n",
       " 1622,\n",
       " 73,\n",
       " 14,\n",
       " 88,\n",
       " 3019,\n",
       " 11]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Change title words to integers\n",
    "from collections import Counter\n",
    "\n",
    "all_text = ' '.join(abstracts_clean) ## Create compiled text\n",
    "words = all_text.split() ## Create list of all words\n",
    "\n",
    "counts = Counter(words) ## Count the occurence of each word\n",
    "vocab = sorted(counts, key=counts.get, reverse=True) ## Sorted words by occurence\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} ## Conversion dictionary\n",
    "int_to_vocab = {ii: word for ii, word in enumerate(vocab, 1)} ## Inversion dictionary\n",
    "\n",
    "abstracts_ints = [] ## Empty titles list for integer encoding\n",
    "for abstract in abstracts_clean:\n",
    "    abstracts_ints.append([vocab_to_int[word] for word in abstract.split()])\n",
    "    \n",
    "## Inspect titles_ints\n",
    "abstracts_ints[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data and one-hot-encode subjects\n",
    "In this section the data can be subseted in order to control the number of subjects to be analyzed. NOTE: For this case, subjects for a given field are being lumped together in one catergory. That is, all subjects of astrophysics are being encoded to fit in the astrophysics group and so forth. This results in 13 distinct 'subjects' to be analyzed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "# Create subset for titles and subjects if needed\n",
    "sub_idx = 15000 ## Subset final index\n",
    "abstracts_ints_sub = abstracts_ints[:sub_idx] ## Titles ints subset\n",
    "labels_sub = labels[:sub_idx] ## Subject subset\n",
    "\n",
    "# Encode 13 \"subjects\" (disciplines) from text to numbers\n",
    "labels_class_sub = np.array([1 if 'astro' in label else 2 if 'physics.' in label\\\n",
    "                            else 3 if 'gr-' in label else 4 if 'hep' in label\\\n",
    "                            else 5 if 'math' in label else 6 if 'nlin.' in label\\\n",
    "                            else 7 if 'nucl-' in label else 8 if 'cond-mat.' in label\\\n",
    "                            else 9 if 'q-bio.' in label else 10 if 'q-fin.' in label\\\n",
    "                            else 11 if 'quant-ph' in label else 12 if 'stat.' in label\n",
    "                            else 0 for label in labels_sub]) \n",
    "                            ## cs has to be labeled 0 to avoid overlap w/ physics\n",
    "\n",
    "print(np.unique(labels_class_sub)) ## check which subjects are being identified\n",
    "\n",
    "# One hot encode of subjects \n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(labels_class_sub)\n",
    "labels_ints_sub = lb.transform(labels_class_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find longest title and create titles matrix with corresponding dimensions\n",
    "It is necessary to find the longest title from the subset in order to determine the number of columns necessary in titles matrix. In cases where the titles are shorter, it is necessary to pad the empty locations in a given row with zeros. All rows are filled from right to left so the padding of zeros occurs prior to each sentence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     8,    56,     3,\n",
       "        7555,    22,     2,   667,   294,    19,  2930,  1213,     1,\n",
       "         157,  2862,  1622,    73,     2,   106,  2265,  9746,  3590,\n",
       "         852, 20853,     4,  1622,   113,     2, 10130,  8079,     8,\n",
       "         140,    59,    51,    10,    71,     1, 39649,  4483,     4,\n",
       "       58985,  7412,  2141,   199,   113,     6,   902,     1,  4476,\n",
       "           2,  1109,   557,  5191,     1, 10130,  1622,   113,    24,\n",
       "        1220,   920,    14,    88,  3019,    11,    79,   961,     2,\n",
       "           1, 20853,  7206, 10130,   908,    11,  2817,   630,  1640,\n",
       "        1551,    67,     1,  2055,   630,  1640,     2,     1,  2344,\n",
       "        1396,  2221,   134,     1,   157,  1622,    73,    24,  1220,\n",
       "        1057,    21,    18,  3019,    79,    64,     8,  1615,    10,\n",
       "        1517,    55, 20853,    14, 10130,  8079,   240,    23,  1227,\n",
       "          10,   920,  1220,  1057,  5421,     1,  4526,    64,     3,\n",
       "        1535,   522,  1273,     5,     1,   332,    85,     2,     1,\n",
       "         106,  2265,   852,    16, 10130,  5004,   692,    13,  1393,\n",
       "           4,   492,  5591,     2,     1,  6492,   934,   973,  5569,\n",
       "          12,   334,   339,    10,    13,    17,   397,     1,  5569,\n",
       "          24,  1220,  1057,   334,    18,   544,  5226,    65,  9303,\n",
       "           4,   871,    65,   141,   568,    69,    55,     2,    27,\n",
       "         148,  1433,  3126,  5004,     2,     1, 10130,  1891,    24,\n",
       "        1220,  1057,     1,   157, 10130,  1622,    73,    14,    88,\n",
       "        3019,    11])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore for now and assume 200 word length\n",
    "# Find length of longest abstract\n",
    "#len_abstracts = []\n",
    "#for i in np.arange(0, len(abstracts_ints_sub), 1):\n",
    "#    len_abstracts.append(len(abstracts_ints_sub[i]))\n",
    "\n",
    "# Create a matrix of titles\n",
    "seq_len = 200 ## max(len_abstracts)\n",
    "features = np.zeros((len(abstracts_ints_sub), seq_len), dtype=int)\n",
    "for i, row in enumerate(abstracts_ints_sub):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "# Inspect features\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and split the data into training, validation, and test sets\n",
    "First, a function is created to do a shuffle-split of the data. Next, a training set of 80% of the original dataset is created. The remaning 20% of the original data is split evenly to create a validation set for model optimization and a test set for final testing. It is important to shuffle the original dataset since it was listed in order of subjects. This is the last preprocessing step necessary to feed our data into TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Function to shuffl and split original dataset\n",
    "from sklearn import cross_validation\n",
    "\n",
    "def shuffle_split(X, y):\n",
    "    \n",
    "    ss = cross_validation.ShuffleSplit(len(labels_ints_sub), n_iter=5, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in ss:\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(12000, 200) \n",
      "Validation set: \t(1500, 200) \n",
      "Test set: \t\t(1500, 200)\n"
     ]
    }
   ],
   "source": [
    "# Create first split for training and test sets\n",
    "X_train, y_train, X_test, y_test = shuffle_split(features, labels_ints_sub)\n",
    "\n",
    "# Split test set to validation and final test sets\n",
    "test_idx = int(len(y_test)*0.5)\n",
    "X_val, y_val = X_test[test_idx:], y_test[test_idx:] ## For validation\n",
    "X_tst, y_tst = X_test[:test_idx], y_test[:test_idx] ## For final testing\n",
    "\n",
    "# Print final shapes\n",
    "print('\\t\\t\\tFeature Shapes:')\n",
    "print('Train set: \\t\\t{}'.format(X_train.shape),\n",
    "     '\\nValidation set: \\t{}'.format(X_val.shape),\n",
    "     '\\nTest set: \\t\\t{}'.format(X_tst.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the TensorFlow Graph\n",
    "Here, we'll build the graph. TensorFlow represents computations as a graph composed of nodes, where a node in a graph is an operation that converts zero or more Tensors and computes zero or more Tensors. \n",
    "\n",
    "First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. One is a good starting point and this code needs to be updated to add more lstm_layer. Therefore, we will keep one layer fixed for now.\n",
    "* `batch_size`: The number of titles to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `learning_rate`: Learning rate determines how \"quickly\" the optimizers will explore the solution space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Key hyperparameters\n",
    "lstm_size = 128\n",
    "lstm_layers = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and initialize input placeholders\n",
    "Constants descriptive of the dataset need to be defined for later use in the training. Also, in TensorFlow it is necessary to create *placeholders* for inputs into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants needed to account for data properties\n",
    "n_words = len(vocab_to_int)\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs') ## Titles\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels') ## Subjects\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob') ## Drop probablity during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding layer\n",
    "It is inefficient to one-hot encode all words in the vocabulary. Therefore, an embedding lookup matrix is created to get the embedded vectors to pass to the LSTM cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 75 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1)) ## Lookup matrix\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_) ## Pass embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM cell and run data through RNN nodes\n",
    "An LSTM cell is created with dropout and the possibility of adding multiple layers if needed. The final 'cell' is then sent through the RNN nodes with *dynamic_rnn*, where TensorFlow takes care of the unrolling and time step computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Run cell through RNN\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate predictions and validation accuracy from outputs\n",
    "By using the final outputs, it is possible then to predict the subjects for each title. To gauage the accuracy of the RNN, a validation accuracy is determined using the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fully_connected/Sigmoid:0\", shape=(100, 13), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create predictions and calculate and optimize cost\n",
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], n_classes, activation_fn=tf.sigmoid)\n",
    "    print(predictions) ## Check dimensions; need to match bath size and subject types\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy for validation \n",
    "with graph.as_default():\n",
    "    #correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    # Need to account for multiple classifiers\n",
    "    correct_pred = tf.equal((tf.arg_max(tf.round(predictions), 1)), (tf.arg_max(labels_, 1)))    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make batches and train RNN\n",
    "Create a function to create batchs of arbitrary size and igoner any entries left over. These batches will then be used to train the RNN and validate accuracy on the validation sets. A checkpoint file is created to record the neural network training and then can be used on the final test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create batches\n",
    "def get_batches(x, y, batch_size=10):\n",
    "    \n",
    "    n_batches = len(x)//batch_size ## Removes left over rows\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20 Iteration: 100 Train loss: 0.062\n",
      "Val acc: 0.239\n",
      "Epoch: 1/20 Iteration: 200 Train loss: 0.055\n",
      "Val acc: 0.351\n",
      "Epoch: 2/20 Iteration: 300 Train loss: 0.045\n",
      "Val acc: 0.370\n",
      "Epoch: 3/20 Iteration: 400 Train loss: 0.041\n",
      "Val acc: 0.399\n",
      "Epoch: 4/20 Iteration: 500 Train loss: 0.041\n",
      "Val acc: 0.404\n",
      "Epoch: 4/20 Iteration: 600 Train loss: 0.036\n",
      "Val acc: 0.431\n",
      "Epoch: 5/20 Iteration: 700 Train loss: 0.021\n",
      "Val acc: 0.465\n",
      "Epoch: 6/20 Iteration: 800 Train loss: 0.023\n",
      "Val acc: 0.456\n",
      "Epoch: 7/20 Iteration: 900 Train loss: 0.025\n",
      "Val acc: 0.453\n",
      "Epoch: 8/20 Iteration: 1000 Train loss: 0.020\n",
      "Val acc: 0.472\n",
      "Epoch: 9/20 Iteration: 1100 Train loss: 0.023\n",
      "Val acc: 0.463\n",
      "Epoch: 9/20 Iteration: 1200 Train loss: 0.021\n",
      "Val acc: 0.467\n",
      "Epoch: 10/20 Iteration: 1300 Train loss: 0.014\n",
      "Val acc: 0.476\n",
      "Epoch: 11/20 Iteration: 1400 Train loss: 0.016\n",
      "Val acc: 0.470\n",
      "Epoch: 12/20 Iteration: 1500 Train loss: 0.014\n",
      "Val acc: 0.467\n",
      "Epoch: 13/20 Iteration: 1600 Train loss: 0.010\n",
      "Val acc: 0.477\n",
      "Epoch: 14/20 Iteration: 1700 Train loss: 0.020\n",
      "Val acc: 0.465\n",
      "Epoch: 14/20 Iteration: 1800 Train loss: 0.012\n",
      "Val acc: 0.488\n",
      "Epoch: 15/20 Iteration: 1900 Train loss: 0.011\n",
      "Val acc: 0.483\n",
      "Epoch: 16/20 Iteration: 2000 Train loss: 0.013\n",
      "Val acc: 0.474\n",
      "Epoch: 17/20 Iteration: 2100 Train loss: 0.012\n",
      "Val acc: 0.485\n",
      "Epoch: 18/20 Iteration: 2200 Train loss: 0.007\n",
      "Val acc: 0.489\n",
      "Epoch: 19/20 Iteration: 2300 Train loss: 0.014\n",
      "Val acc: 0.482\n",
      "Epoch: 19/20 Iteration: 2400 Train loss: 0.010\n",
      "Val acc: 0.493\n"
     ]
    }
   ],
   "source": [
    "# Typical trainig code for TensorFlow\n",
    "\n",
    "epochs = 20 ## Number of full iterations\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(X_train, y_train, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: np.array(y),\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%100==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "            if iteration%100==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(X_val, y_val, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: np.array(y),\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "\n",
    "    saver.save(sess, \"checkpoints/subjects.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RNN on final test set\n",
    "This code runs final test set to determing final accuracy of trained RNN network. It is important to have a directory where the checkpoint file may be stored for this last testing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/subjects.ckpt\n",
      "Test accuracy: 0.512\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(X_tst, y_tst, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y,\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
